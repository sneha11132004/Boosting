{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BOOSTING"
      ],
      "metadata": {
        "id": "FgHuUqOL4pJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.: What is the fundamental idea behind ensemble techniques? How does\n",
        "bagging differ from boosting in terms of approach and objective?\n",
        "\n",
        "- The fundamental idea behind ensemble techniques is to combine multiple weak or base learners (like decision trees) to create a more accurate, robust, and generalized model. By aggregating predictions from several models, ensemble methods reduce variance, bias, and improve overall predictive performance.\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Approach: Trains multiple models independently on different random subsets of the training data (created through bootstrapping).\n",
        "Objective: Reduces variance and prevents overfitting.\n",
        "Example: Random Forest.\n",
        "Combination: Uses averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Approach: Trains models sequentially, where each new model focuses on correcting the errors of the previous one.\n",
        "Objective: Reduces bias and improves accuracy by building strong learners from weak ones.\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
        "Combination: Weighted sum of model outputs.\n"
      ],
      "metadata": {
        "id": "NLPaLBn64r7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain how the Random Forest Classifier reduces overfitting compared to\n",
        "a single decision tree. Mention the role of two key hyperparameters in this process?\n",
        "\n",
        "- The Random Forest Classifier reduces overfitting by combining multiple decision trees trained on random subsets of data and features. This randomness ensures that individual trees are less correlated and capture different patterns, making the ensemble more generalizable and less prone to overfitting.\n",
        "\n",
        "Two key hyperparameters that help in this process are:\n",
        "\n",
        "1. n_estimators:\n",
        "\n",
        "The number of trees in the forest.\n",
        "A higher number generally improves performance and stability by averaging out errors from individual trees.\n",
        "\n",
        "2. max_features:\n",
        "\n",
        "The number of features to consider when splitting a node.\n",
        "Using fewer random features for each tree increases diversity among trees, reducing correlation and overfitting.\n",
        "Together, these parameters create a balance between bias and variance, improving overall model generalization.\n"
      ],
      "metadata": {
        "id": "lELpFZ3949dL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.: What is Stacking in ensemble learning? How does it differ from traditional\n",
        "bagging/boosting methods? Provide a simple example use case?\n",
        "\n",
        "Stacking (Stacked Generalization) is an ensemble learning technique where predictions from multiple base models (level-1 models) are combined using a meta-model (level-2 model) that learns the optimal way to blend their outputs.\n",
        "\n",
        "Difference from Bagging/Boosting:\n",
        "Bagging trains models independently and averages their predictions.\n",
        "Boosting trains models sequentially to correct previous errors.\n",
        "Stacking combines diverse models (e.g., Decision Tree, Logistic Regression, SVM) and uses another model (e.g., Linear Regression) to learn the best combination of their outputs.\n",
        "\n",
        "Example Use Case:\n",
        "In a loan approval prediction system, stacking could combine:\n",
        "A Random Forest (captures non-linear relationships),\n",
        "A Logistic Regression (captures linear trends),\n",
        "And a Gradient Boosting model.\n",
        "A meta-model like Linear Regression would then learn how to weight these models’ predictions to achieve the best overall accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qgOI2Odx5Yax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.:What is the OOB Score in Random Forest, and why is it useful? How does\n",
        "it help in model evaluation without a separate validation set?\n",
        "\n",
        "-The OOB (Out-Of-Bag) score is an internal performance estimate used in Random Forests. It measures how well the model predicts unseen data without requiring a separate validation or test set.\n",
        "\n",
        "Explanation:\n",
        "When building each tree in a Random Forest, only a bootstrap sample (random sample with replacement) of the training data is used. On average, about 63% of the training instances are used to train the tree, while the remaining 37% of the data (called Out-Of-Bag samples) are left out.\n",
        "These OOB samples serve as unseen data for that particular tree. Once the forest is trained, each sample’s prediction is averaged from all trees where it was OOB, and accuracy is calculated over the entire dataset — this gives the OOB score.\n",
        "\n",
        "Why it is useful:\n",
        "It provides an unbiased estimate of model performance similar to cross-validation.\n",
        "It saves time and data since no additional validation set is needed.\n",
        "It helps in tuning hyperparameters efficiently during training.\n",
        "\n",
        "In short:\n",
        "OOB score acts like an internal cross-validation mechanism, giving a reliable estimate of how well the Random Forest generalizes to unseen data.\n"
      ],
      "metadata": {
        "id": "LfAtjOzc5oGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.: Compare AdaBoost and Gradient Boosting in terms of:\n",
        "● How they handle errors from weak learners\n",
        "● Weight adjustment mechanism\n",
        "● Typical use cases\n",
        "\n",
        "- Aspect\tAdaBoost\tGradient Boosting\n",
        "\n",
        "How they handle errors from weak learners\tAdaBoost increases the weights of misclassified samples so that the next weak learner focuses more on those difficult samples.\tGradient Boosting fits the next learner to the residual errors (difference between actual and predicted values) of the previous model.\n",
        "Weight adjustment mechanism\tSample weights are adjusted exponentially based on classification errors — higher weight to incorrectly predicted samples.\tNo explicit sample weighting. Errors are minimized by using gradient descent to update predictions in the direction of reducing loss.\n",
        "Typical use cases\tMostly used for classification problems (binary or multiclass). Example: Spam detection, face recognition.\tUsed for both regression and classification. Example: Predicting house prices, customer churn, etc."
      ],
      "metadata": {
        "id": "HSFpy5Fz6cRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.6:Why does CatBoost perform well on categorical features without requiring\n",
        "extensive preprocessing? Briefly explain its handling of categorical variables?\n",
        "\n",
        "-CatBoost automatically handles categorical variables using a technique called ordered target statistics (or target encoding with permutations).\n",
        "It replaces categorical values with numerical representations based on how the target variable behaves for each category, while preventing data leakage by using permutations.\n",
        "This eliminates the need for manual encoding (like one-hot or label encoding) and reduces overfitting, allowing CatBoost to work efficiently with categorical data."
      ],
      "metadata": {
        "id": "Bt8ye0k_6wJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7.: KNN Classifier Assignment: Wine Dataset Analysis with\n",
        "#Optimization\n",
        "#Task:\n",
        "#1. Load the Wine dataset (sklearn.datasets.load_wine()).\n",
        "#2. Split data into 70% train and 30% test.\n",
        "#3. Train a KNN classifier (default K=5) without scaling and evaluate using:\n",
        "#a. Accuracy\n",
        "#b. Precision, Recall, F1-Score (print classification report)\n",
        "#4. Apply StandardScaler, retrain KNN, and compare metrics.\n",
        "#5. Use GridSearchCV to find the best K (test K=1 to 20) and distance metric\n",
        "#(Euclidean, Manhattan).\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train KNN (default k=5) without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"=== Without Scaling ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 4. Apply StandardScaler and retrain KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n=== With Scaling ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_scaled))\n",
        "print(classification_report(y_test, y_pred_scaled))\n",
        "\n",
        "# 5. Use GridSearchCV to find best K (1 to 20) and distance metric\n",
        "param_grid = {\n",
        "    'n_neighbors': range(1, 21),\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    KNeighborsClassifier(),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\n=== Best Parameters from GridSearchCV ===\")\n",
        "print(grid.best_params_)\n",
        "print(\"Best Cross-validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# 6. Train optimized KNN with best params and compare\n",
        "best_knn = grid.best_estimator_\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n=== Optimized KNN on Scaled Data ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(classification_report(y_test, y_pred_best))"
      ],
      "metadata": {
        "id": "9-9wJi8o7TPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.: PCA + KNN with Variance Analysis and Visualization\n",
        "#Task:\n",
        "#1. Load the Breast Cancer dataset (sklearn.datasets.load_breast_cancer()).\n",
        "#2. Apply PCA and plot the scree plot (explained variance ratio).\n",
        "#3. Retain 95% variance and transform the dataset.\n",
        "#4. Train KNN on the original data and PCA-transformed data, then compare\n",
        "#accuracy.\n",
        "#5. Visualize the first two principal components using a scatter plot (color by class)\n",
        "\n",
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale data before PCA or KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply PCA and plot explained variance ratio\n",
        "pca = PCA().fit(X_train_scaled)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_)+1),\n",
        "         pca.explained_variance_ratio_, marker='o')\n",
        "plt.title(\"Scree Plot – Explained Variance Ratio\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. Retain 95% variance\n",
        "pca_95 = PCA(0.95)\n",
        "X_train_pca = pca_95.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_95.transform(X_test_scaled)\n",
        "print(f\"Number of components to retain 95% variance: {pca_95.n_components_}\")\n",
        "\n",
        "# 5. Train KNN on original (scaled) data\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_orig = knn_original.predict(X_test_scaled)\n",
        "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
        "print(\"\\n=== KNN on Original Scaled Data ===\")\n",
        "print(\"Accuracy:\", acc_orig)\n",
        "print(classification_report(y_test, y_pred_orig))\n",
        "\n",
        "# 6. Train KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"\\n=== KNN on PCA-Transformed Data ===\")\n",
        "print(\"Accuracy:\", acc_pca)\n",
        "print(classification_report(y_test, y_pred_pca))\n",
        "\n",
        "# 7. Compare\n",
        "print(f\"\\nAccuracy Comparison:\\nOriginal = {acc_orig:.4f}, PCA = {acc_pca:.4f}\")\n",
        "\n",
        "# 8. Visualize first two principal components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_2D = pca_2.fit_transform(X_train_scaled)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(X_2D[:,0], X_2D[:,1], c=y_train, cmap='coolwarm', alpha=0.7)\n",
        "plt.title(\"PCA – First Two Principal Components\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.colorbar(label='Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdobOmK17l6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkSeUr-R164M"
      },
      "outputs": [],
      "source": [
        "#9.KNN Regressor with Distance Metrics and K-Value\n",
        "#Analysis\n",
        "#Task:\n",
        "#1. Generate a synthetic regression dataset\n",
        "#(sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
        "#2. Train a KNN regressor with:\n",
        "#a. Euclidean distance (K=5)\n",
        "#b. Manhattan distance (K=5)\n",
        "#c. Compare Mean Squared Error (MSE) for both.\n",
        "#3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff.\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=10, noise=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale data for distance-based models\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Train KNN Regressor with Euclidean and Manhattan (K=5)\n",
        "knn_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n",
        "knn_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_euc = knn_euclidean.predict(X_test_scaled)\n",
        "y_pred_man = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "mse_euc = mean_squared_error(y_test, y_pred_euc)\n",
        "mse_man = mean_squared_error(y_test, y_pred_man)\n",
        "\n",
        "print(\"=== MSE Comparison (K=5) ===\")\n",
        "print(\"Euclidean Distance:\", round(mse_euc, 3))\n",
        "print(\"Manhattan Distance:\", round(mse_man, 3))\n",
        "\n",
        "# 3. Test K = 1–20 and plot K vs MSE\n",
        "k_values = range(1, 21)\n",
        "mse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean')\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    mse_values.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(k_values, mse_values, marker='o')\n",
        "plt.title(\"K vs Mean Squared Error (Bias–Variance Tradeoff)\")\n",
        "plt.xlabel(\"Number of Neighbors (K)\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_k = k_values[np.argmin(mse_values)]\n",
        "print(f\"Best K based on lowest MSE: {best_k}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.KNN with KD-Tree/Ball Tree, Imputation, and Real-World\n",
        "#Data\n",
        "#Task:\n",
        "#1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "#2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
        "#3. Train KNN using:\n",
        "#a. Brute-force method\n",
        "#b. KD-Tree\n",
        "#c. Ball Tree\n",
        "#4. Compare their training time and accuracy.\n",
        "#5. Plot the decision boundary for the best-performing method (use 2 most important\n",
        "#Features).\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load Pima Indians Diabetes dataset\n",
        "# (Dataset link: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "cols = [\n",
        "    \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
        "    \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"\n",
        "]\n",
        "df = pd.read_csv(url, names=cols)\n",
        "\n",
        "# 2. Replace invalid 0 values with NaN (for selected features)\n",
        "features_with_zero = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
        "df[features_with_zero] = df[features_with_zero].replace(0, np.nan)\n",
        "\n",
        "# Apply KNN Imputer to fill missing values\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=cols)\n",
        "\n",
        "# Split data\n",
        "X = df_imputed.drop(\"Outcome\", axis=1)\n",
        "y = df_imputed[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train KNN using different algorithms\n",
        "methods = [\"brute\", \"kd_tree\", \"ball_tree\"]\n",
        "results = {}\n",
        "\n",
        "for method in methods:\n",
        "    start = time.time()\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=method)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    end = time.time()\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[method] = {\"Accuracy\": acc, \"Time (s)\": end - start}\n",
        "    print(f\"\\n=== {method.upper()} ===\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"Training + Prediction Time:\", round(end - start, 4), \"seconds\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 4. Compare performance\n",
        "result_df = pd.DataFrame(results).T\n",
        "print(\"\\nPerformance Comparison:\\n\", result_df)\n",
        "\n",
        "# 5. Plot decision boundary (using 2 most important features)\n",
        "# Let's pick 'Glucose' and 'BMI'\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X2 = df_imputed[[\"Glucose\", \"BMI\"]]\n",
        "y2 = df_imputed[\"Outcome\"]\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
        "scaler2 = StandardScaler()\n",
        "X2_train_scaled = scaler2.fit_transform(X2_train)\n",
        "X2_test_scaled = scaler2.transform(X2_test)\n",
        "\n",
        "# Use best-performing method (from result_df)\n",
        "best_method = result_df[\"Accuracy\"].idxmax()\n",
        "best_knn = KNeighborsClassifier(n_neighbors=5, algorithm=best_method)\n",
        "best_knn.fit(X2_train_scaled, y2_train)\n",
        "\n",
        "# Create mesh grid for plotting\n",
        "x_min, x_max = X2_train_scaled[:, 0].min() - 1, X2_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X2_train_scaled[:, 1].min() - 1, X2_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = best_knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA','#AAFFAA']), alpha=0.6)\n",
        "plt.scatter(X2_train_scaled[:,0], X2_train_scaled[:,1], c=y2_train, cmap='bwr', edgecolors='k', alpha=0.8)\n",
        "plt.title(f\"KNN Decision Boundary ({best_method.upper()} method)\")\n",
        "plt.xlabel(\"Glucose (scaled)\")\n",
        "plt.ylabel(\"BMI (scaled)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9KCT8Svb-EAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}